nameNode=hdfs://hadoop2:8020
jobTracker=hadoop2:8032
queueName=default

#oozie.libpath=${nameNode}/user/${user.name}/share/lib/lib_20161121154957/sqoop
oozie.use.system.libpath=true
workflowAppUri=hdfs://hadoop2:8020/spark/oozie_ztyth_log
hive_metastore_uris=thrift://hadoop2:9083



##spark=============================================
#nameNode=hdfs://hadoop3:8020
#jobTracker=hadoop3:8032
master=yarn
mode=cluster
#hive_metastore_warehouse_dir=/user/hive/warehouse
#oozie.use.system.libpath=true

#oozie.wf.application.path=hdfs://hadoop3:8020/spark/oozie_spark_demo/workflow.xml
#oozie.libpath=hdfs://hadoop3:8020/user/oozie/share/lib/release/spark/jars
oozie.action.sharelib.for.spark=spark2
security_enabled=False
dryrun=False


##编辑区域============================================
#全流程跑
startNode=sqoop2hive_T_ZTYTH_HB_HEAD
#单解析
#startNode=spark_node

#spark 1st参数  debug 为debug（不进行文档归档，并且读取debug的测试log，默认路径为/data/logHDFS/apollo/test.log
firstParam=debug
secondParam=/data/logHDFS/apollo/test.log

# 开始时间 注意时间为UTC时间，需要北京时间-8小时
startDate=2018-08-07T01:50Z
endDate=2088-07-26T16:00Z
# 直接执行
oozie.wf.application.path=hdfs://hadoop2:8020/spark/oozie_ztyth_log/workflow.xml
#oozie.coord.application.path=hdfs://hadoop2:8020/spark/oozie_ztyth_log/coordinator.xml

#凌晨10分钟开始执行
coorfrequency=10 0 * * *

sqoop_import_params=--connect "jdbc:oracle:thin:@192.168.10.128:1521:orcl" --username ztyth_qy --password dcjet -hive-drop-import-delims --fields-terminated-by "\001" --hive-overwrite --delete-target-dir --hive-import -m 5 
sqoop_import_params_short=--connect "jdbc:oracle:thin:@192.168.10.128:1521:orcl" --username ztyth_qy --password dcjet