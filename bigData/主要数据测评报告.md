集群情况：
cpu:i7 8700 内存：8G2条共16G 固态硬盘120G +机械硬盘：3T  *3


##sqoop导入情况
sqoop import --connect 'jdbc:sqlserver://192.168.12.240:1433;username=dcjet240;password=Dcjet$888;database=HA2000_BD' --table ENTRY_HEAD_BD --hive-import -m 30  --split-by ENTRY_ID  -hive-drop-import-delims --fields-terminated-by "\001" --verbose
###1e   entry_head  花费时间：  3hrs, 48mins, 4sec 100M/s

sqoop import --connect 'jdbc:sqlserver://192.168.12.240:1433;username=dcjet240;password=Dcjet$888;database=HA2000_BD' --table ENTRY_WORKFLOW_BD --hive-import -m 30  --split-by ENTRY_ID  -hive-drop-import-delims --fields-terminated-by "\001" --verbose
###10e  workflow    花费时间：  4hrs, 14mins, 3sec  100M/s




sqoop import --connect 'jdbc:sqlserver://192.168.12.240:1433;username=dcjet240;password=Dcjet$888;database=HA2000_BD' --table ENTRY_LIST_BD --hive-import -m 30  --split-by ENTRY_ID  -hive-drop-import-delims --fields-terminated-by "\001" --verbose
Transferred 387.3538 GB in 54,935.6573 seconds (7.2203 MB/sec)

##查询效率
hive: select count(1) from entry_workflow   8e数据   8.5 min
spark: select count(1) from entry_workflow   8e数据 7.2 min


bd数据查询，数据量  head 1e  workfolw 8e   结果：47439200
mssql: 查不出
spark： 16min(count)   	18 min(save to hdfs)

##
中间库 
三家企业 1.4 h   数据总量 5k万条数据，后续数据采用impala直接方法

